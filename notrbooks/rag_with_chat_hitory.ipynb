{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat History Yönetimli RAG Chatbot Uygulaması\n",
    "\n",
    "Yüklediğiniz PDF/TXT/DOCX belgelerden bilgi retrieve eden, OpenAI destekli RAG Chatbot örneği. \n",
    "Ayrıca, kullanıcı sohbet geçmişini kaydeder, arayabilir, temizleyebilir ve tekrar yükleyebilirsiniz.\n",
    "\n",
    "### Kullanacağımız teknoloji stack'i:\n",
    "- OpenAI (chat ve embedding modelleri)\n",
    "- FAISS (vektör tabanlı arama için)\n",
    "- LangChain (framework)\n",
    "- Python (klasik notebook arayüzü) -> daha sonra streamlit\n",
    "- Chat History yönetimi (sadece context olarak değil, örnekleme/analiz için de)\n",
    "\n",
    "### Senaryo\n",
    "- Kullanıcı PDF/DOCX/TXT belgesi yüklüyor.\n",
    "- Belgelerden embedding'ler çıkarılıyor ve FAISS ile arama index'i oluşturuluyor.\n",
    "- Kullanıcı, soru soruyor.\n",
    "- Soru embedding'e dönüştürülüp, vektör arama ile en ilgili context'ler bulunuyor.\n",
    "- Sonrasında OpenAI Chat API'ya hem kullanıcı geçmişi hem de ilgili doküman parçaları eklenip yanıt oluşturuluyor.\n",
    "- Chat geçmişi, her turda context olarak prompt'a ekleniyor ve ayrı bir yerde de saklanıyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gerekli Kütüphanelerin Kurulumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain faiss-cpu tiktoken PyPDF2 python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Kütüphanelerin İçe Aktarılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import getpass\n",
    "import textwrap\n",
    "import datetime\n",
    "# LLM Imports\n",
    "import faiss\n",
    "import openai\n",
    "# Langchain Imports\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document as LCDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Documantation Imports\n",
    "from docx import Document\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. OpenAI API Key Tanımlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key is None:\n",
    "    openai_api_key = getpass.getpass(\"OpenAI API Key'inizi giriniz: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Belge Okuma Fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def load_document(file_path):\n",
    "    if file_path.endswith('.txt'):\n",
    "        return read_txt(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        return read_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        return read_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Desteklenmeyen dosya formatı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Belge Chunk'lama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(document, chunk_size=500, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Chat History Manager Sınıfı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatHistoryManager:\n",
    "    def __init__(self, user_id, base_path=\"chat_histories\"):\n",
    "        self.user_id = user_id\n",
    "        self.base_path = base_path\n",
    "        self.history = []\n",
    "        os.makedirs(self.base_path, exist_ok=True)\n",
    "\n",
    "    def add_message(self, role, message):\n",
    "        self.history.append({\n",
    "            \"role\": role,\n",
    "            \"message\": message,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    def get_history(self, n_last=None):\n",
    "        if n_last is None:\n",
    "            return self.history\n",
    "        return self.history[-n_last:]\n",
    "\n",
    "    def save(self):\n",
    "        file_path = os.path.join(self.base_path, f\"{self.user_id}_history.json\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.history, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load(self):\n",
    "        file_path = os.path.join(self.base_path, f\"{self.user_id}_history.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.history = json.load(f)\n",
    "        else:\n",
    "            self.history = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.history = []\n",
    "        self.save()\n",
    "\n",
    "    def search(self, keyword):\n",
    "        return [msg for msg in self.history if keyword.lower() in msg[\"message\"].lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Belge Yükleme\n",
    "\n",
    "**Not:** Burada notebook çalıştırdığınız yerde bir dosya olması lazım.\n",
    "PDF/TXT/DOCX dosyanızın adını aşağıda belirtin (ör: example.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Profile.pdf\"\n",
    "document = load_document(file_path)\n",
    "chunks = chunk_document(document)\n",
    "print(f\"{len(chunks)} adet chunk oluşturuldu.\")\n",
    "\n",
    "# Her chunk için kaynak bilgisini hazırla\n",
    "sources = [f\"Chunk {i+1}: {chunk[:60].replace('\\n', ' ')}...\" for i, chunk in enumerate(chunks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. FAISS Index ve Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "docs_with_metadata = [\n",
    "    LCDocument(page_content=chunk, metadata={\"source\": sources[i]})\n",
    "    for i, chunk in enumerate(chunks)\n",
    "]\n",
    "db = FAISS.from_documents(docs_with_metadata, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Kullanıcı Tanımlama ve Chat Geçmişi Yönetimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = str(uuid.uuid4())[:8]  # otomatik ve benzersiz id\n",
    "print(f\"Oturumunuz için otomatik atanmış kullanıcı id: {user_id}\")\n",
    "history_manager = ChatHistoryManager(user_id)\n",
    "history_manager.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Retrieval ve OpenAI API Fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_docs_with_source(query, db, k=4, threshold=0.45):\n",
    "    docs_and_scores = db.similarity_search_with_score(query, k=k)\n",
    "    filtered = [\n",
    "        (doc.page_content, doc.metadata.get(\"source\", \"kaynak yok\"))\n",
    "        for doc, score in docs_and_scores\n",
    "        # NOT: FAISS skorları genellikle mesafe, daha küçük = daha yakın. \n",
    "        # Burada 0.4 gibi düşük threshold daha \"sıkı\" seçimdir.\n",
    "        if score < threshold # eşik altında ise gerçekten ilgili chunk kabul et\n",
    "    ]\n",
    "    docs, sources = zip(*filtered) if filtered else ([], [])\n",
    "    return list(docs), list(sources)\n",
    "\n",
    "def format_context_with_sources(docs, sources):\n",
    "    if not docs:\n",
    "        return \"(Bu soruyla ilgili kaynak chunk bulunamadı.)\"\n",
    "    return \"\\n\\n\".join([f\"[{source}]\\n{doc}\" for source, doc in zip(sources, docs)])\n",
    "\n",
    "def format_chat_history(history):\n",
    "    return \"\\n\".join([f\"{role}: {msg}\" for role, msg in history[-6:]])\n",
    "\n",
    "def build_chat_prompt_with_sources(user_input, docs, sources, chat_history):\n",
    "    context = format_context_with_sources(docs, sources)\n",
    "    history_str = format_chat_history(chat_history)\n",
    "    prompt = f\"\"\"\n",
    "        Aşağıda kullanıcı sorusuyla ilgili belge parçaları bulunmaktadır. Bunlara dayalı şekilde, net, referanslı ve açıklayıcı bir yanıt ver.\n",
    "        \n",
    "        Belge Parçaları:\n",
    "        {context}\n",
    "        \n",
    "        Geçmiş Sohbet:\n",
    "        {history_str}\n",
    "        \n",
    "        Kullanıcı Sorusu:\n",
    "        {user_input}\n",
    "    \"\"\"\n",
    "    return textwrap.dedent(prompt).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Chatbot Döngüsü (Chat History Yönetimi ile)\n",
    "\n",
    "**Komutlar:**\n",
    "- **history:** Tüm chat geçmişini gösterir\n",
    "- **clear:** Sohbet geçmişini siler\n",
    "- **search kelime:** Geçmişte anahtar kelime arar\n",
    "- **quit veya exit:** Sohbeti bitirir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI istemcisiyle başlat \n",
    "client = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nSen: \").strip()\n",
    "    print(\"\\nSen:\", user_input)\n",
    "    user_input_lower = user_input.lower()\n",
    "\n",
    "    if user_input_lower in {\"quit\", \"exit\"}:\n",
    "        print(\"Çıkılıyor.\")\n",
    "        break\n",
    "\n",
    "    if user_input_lower == \"history\":\n",
    "        print(\"\\n--- Sohbet Geçmişi ---\")\n",
    "        for msg in history_manager.get_history():\n",
    "            print(f\"{msg['role']} ({msg['timestamp']}): {msg['message']}\")\n",
    "        continue\n",
    "\n",
    "    if user_input_lower == \"clear\":\n",
    "        history_manager.clear()\n",
    "        print(\"Sohbet geçmişi temizlendi.\")\n",
    "        continue\n",
    "\n",
    "    if user_input_lower.startswith(\"search \"):\n",
    "        keyword = user_input.split(\" \", 1)[1]\n",
    "        results = history_manager.search(keyword)\n",
    "        print(f\"\\n--- '{keyword}' için geçmiş mesajlar ---\")\n",
    "        for msg in results:\n",
    "            print(f\"{msg['role']} ({msg['timestamp']}): {msg['message']}\")\n",
    "        continue\n",
    "\n",
    "    # RAG: En alakalı chunk'ları al\n",
    "    relevant_docs, sources = get_relevant_docs_with_source(user_input, db)\n",
    "\n",
    "    # Geçmişi al ve prompt'u oluştur\n",
    "    history_last = [(msg[\"role\"], msg[\"message\"]) for msg in history_manager.get_history(6)]\n",
    "    prompt_text = build_chat_prompt_with_sources(user_input, relevant_docs, sources, history_last)\n",
    "    \n",
    "    # messages dizisi: system + context + geçmiş + yeni soru\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Sen profesyonel bir yardımcı asistansın. \"\n",
    "                \"Kullanıcı sorularını, verilen belge parçaları ve geçmiş konuşmaya göre yanıtla. \"\n",
    "                \"Kaynaklardan aldığın bilgiye referans vermeyi unutma.\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    \n",
    "    # Assistan yanıtı\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=512,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    assistant_response = response.choices[0].message.content\n",
    "    print(\"\\nAsistan:\", assistant_response)\n",
    "\n",
    "    print(\"\\nKullanılan kaynak chunk'lar:\")\n",
    "    if sources:\n",
    "        for s in sources:\n",
    "            print(\"-\", s)\n",
    "    else:\n",
    "        print(\"- Hiçbir kaynak bulunamadı.\")\n",
    "\n",
    "    # Geçmişe ekle\n",
    "    history_manager.add_message(\"user\", user_input)\n",
    "    history_manager.add_message(\"assistant\", assistant_response)\n",
    "    history_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Sohbet Geçmişini Dışa Aktar\n",
    "\n",
    "Geçmişi bir data frame olarak dışarı aktaralım ve veri analizi için pandas ile okuyalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(history_manager.get_history())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_lecture",
   "language": "python",
   "name": "llm_lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
